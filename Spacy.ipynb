{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The factors that work in the favor of spaCy are the set of features it offers, the ease of use, and the fact\n",
    "#that the library is always kept up to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()\n",
    "#Here, the nlp object is a language model instance.\n",
    "#this is for english language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names\n",
    "#to get an idea what is inside a pipeline components.\n",
    "# the text has to already go through this pipeline and if want only one functionality we can disable the other one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "about_text = ('Ram has a work to do but he is into something'\n",
    "               ' he is working for a London-based Fintech'\n",
    "               ' company. He is interested in learning'\n",
    "               ' Natural Language Processing.')\n",
    "about_doc = nlp(about_text)\n",
    "\n",
    "#sentence detection. If you see even if i have given 4 sentences individually but spacy has formed 3 sentences only.\n",
    "#it has formed sentences based on (.) but that is not true always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Ram has a work to do,\n",
       " but he is into something he is working for a London-based Fintech company.,\n",
       " He is interested in learning Natural Language Processing.]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#i feel that NLTK is exceeding spacy in terms of sentence tokenization.\n",
    "sentences = list(about_doc.sents)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ram\n",
      "has\n",
      "a\n",
      "work\n",
      "to\n",
      "do\n",
      "but\n",
      "he\n",
      "is\n",
      "into\n",
      "something\n",
      "he\n",
      "is\n",
      "working\n",
      "for\n",
      "a\n",
      "London\n",
      "-\n",
      "based\n",
      "Fintech\n",
      "company\n",
      ".\n",
      "He\n",
      "is\n",
      "interested\n",
      "in\n",
      "learning\n",
      "Natural\n",
      "Language\n",
      "Processing\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "#Tokenization \n",
    "for token in about_doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing stopwords\n",
    "#here as we have considered english Language so we are calculating the stop words.\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so the stopwords are removed.\n",
    "list=[]\n",
    "for token in about_doc:\n",
    "    if not token.is_stop:\n",
    "        list.append(token)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Ram,\n",
       " work,\n",
       " working,\n",
       " London,\n",
       " -,\n",
       " based,\n",
       " Fintech,\n",
       " company,\n",
       " .,\n",
       " interested,\n",
       " learning,\n",
       " Natural,\n",
       " Language,\n",
       " Processing,\n",
       " .]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ram --> PROPN\n",
      "work --> NOUN\n",
      "working --> VERB\n",
      "London --> PROPN\n",
      "- --> PUNCT\n",
      "based --> VERB\n",
      "Fintech --> PROPN\n",
      "company --> NOUN\n",
      ". --> PUNCT\n",
      "interested --> ADJ\n",
      "learning --> VERB\n",
      "Natural --> PROPN\n",
      "Language --> PROPN\n",
      "Processing --> PROPN\n",
      ". --> PUNCT\n"
     ]
    }
   ],
   "source": [
    "#pos tagging\n",
    "for token in list:\n",
    "    # Print the token and its part-of-speech tag\n",
    "    print(token.text, \"-->\", token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  --->  \n",
      "Lemmatization ---> Lemmatization\n",
      "is ---> be\n",
      "a ---> a\n",
      "way ---> way\n",
      "of ---> of\n",
      "dealing ---> deal\n",
      "with ---> with\n",
      "the ---> the\n",
      "fact ---> fact\n",
      "that ---> that\n",
      "while ---> while\n",
      "words ---> word\n",
      "like ---> like\n",
      "connect ---> connect\n",
      ", ---> ,\n",
      "connection ---> connection\n",
      ", ---> ,\n",
      "connecting ---> connect\n",
      ", ---> ,\n",
      "connected ---> connect\n",
      ", ---> ,\n",
      "etc.etc ---> etc.etc\n",
      ". ---> .\n",
      "are ---> be\n",
      "n’t ---> not\n",
      "exactly ---> exactly\n",
      "the ---> the\n",
      "same ---> same\n",
      ", ---> ,\n",
      "they ---> -PRON-\n",
      "all ---> all\n",
      "have ---> have\n",
      "the ---> the\n",
      "same ---> same\n",
      "essential ---> essential\n",
      "meaning ---> meaning\n",
      ": ---> :\n",
      "connect ---> connect\n",
      ". ---> .\n",
      "The ---> the\n",
      "differences ---> difference\n",
      "in ---> in\n",
      "spelling ---> spelling\n",
      "have ---> have\n",
      "grammatical ---> grammatical\n",
      "functions ---> function\n",
      "in ---> in\n",
      "spoken ---> spoken\n",
      "language ---> language\n",
      ", ---> ,\n",
      "but ---> but\n",
      "for ---> for\n",
      "machine ---> machine\n",
      "processing ---> processing\n",
      ", ---> ,\n",
      "those ---> those\n",
      "differences ---> difference\n",
      "can ---> can\n",
      "be ---> be\n",
      "confusing ---> confusing\n",
      ", ---> ,\n",
      "so ---> so\n",
      "we ---> -PRON-\n",
      "need ---> need\n",
      "a ---> a\n",
      "way ---> way\n",
      "to ---> to\n",
      "change ---> change\n",
      "all ---> all\n",
      "the ---> the\n",
      "words ---> word\n",
      "that ---> that\n",
      "are ---> be\n",
      "forms ---> form\n",
      "of ---> of\n",
      "the ---> the\n",
      "word ---> word\n",
      "connect ---> connect\n",
      "into ---> into\n",
      "the ---> the\n",
      "word ---> word\n",
      "connect ---> connect\n",
      "itself ---> -PRON-\n",
      ". ---> .\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization\n",
    "file=\"\"\" Lemmatization is a way of dealing with the fact that while words like connect, connection, connecting, connected, etc.etc. aren’t exactly the same, they all have the same essential meaning: connect. The differences in spelling have grammatical functions in spoken language, but for machine processing, those differences can be confusing, so we need a way to change all the words that are forms of the word connect into the word connect itself.\"\"\"\n",
    "file=nlp(file)\n",
    "for token in file:\n",
    "    print (token,\"--->\" ,token.lemma_)\n",
    "#Unlike verbs and common nouns, there’s no clear base form of a personal pronoun.\n",
    "#-PRON-, which is used as the lemma for all personal pronouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#detecting word frequency\n",
    "from collections import Counter\n",
    "file=\"\"\" Lemmatization is a way of dealing with the fact that while words like connect, connection, connecting, connected, etc.etc. aren’t exactly the same, they all have the same essential meaning: connect. The differences in spelling have grammatical functions in spoken language, but for machine processing, those differences can be confusing, so we need a way to change all the words that are forms of the word connect into the word connect itself.\"\"\"\n",
    "file=nlp(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words and punctuation symbols in list comprehension format\n",
    "words = [token.text for token in file if not token.is_stop and not token.is_punct]\n",
    "#there are simple functions in spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq=Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({' ': 1,\n",
       "         'Lemmatization': 1,\n",
       "         'way': 2,\n",
       "         'dealing': 1,\n",
       "         'fact': 1,\n",
       "         'words': 2,\n",
       "         'like': 1,\n",
       "         'connect': 4,\n",
       "         'connection': 1,\n",
       "         'connecting': 1,\n",
       "         'connected': 1,\n",
       "         'etc.etc': 1,\n",
       "         'exactly': 1,\n",
       "         'essential': 1,\n",
       "         'meaning': 1,\n",
       "         'differences': 2,\n",
       "         'spelling': 1,\n",
       "         'grammatical': 1,\n",
       "         'functions': 1,\n",
       "         'spoken': 1,\n",
       "         'language': 1,\n",
       "         'machine': 1,\n",
       "         'processing': 1,\n",
       "         'confusing': 1,\n",
       "         'need': 1,\n",
       "         'change': 1,\n",
       "         'forms': 1,\n",
       "         'word': 2})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['way', 'words', 'differences', 'word']\n"
     ]
    }
   ],
   "source": [
    "#to print the words by giving a frequency count\n",
    "freqwords = [word for (word, freq) in freq.items() if freq == 2]\n",
    "print (freqwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "China GPE\n",
      "over $71 billion MONEY\n",
      "India GPE\n",
      "the year of 2018 DATE\n"
     ]
    }
   ],
   "source": [
    "#named entity recognition\n",
    "#theory given in the document\n",
    "doc = nlp(\"China spent over $71 billion in India in the year of 2018\")\n",
    "# we have the entities \n",
    "for e in doc.ents:\n",
    "    print(e.text, e.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
