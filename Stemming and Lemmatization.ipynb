{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happi'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stemming is the process of reducing \n",
    "#inflected words to their word stem, base or root form—generally a written word form.\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer     \n",
    "stemmerporter=PorterStemmer()         \n",
    "stemmerporter.stem('happiness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happy'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer #It is more efficient as compared to porter stemmer\n",
    "stemmerLan=LancasterStemmer()\n",
    "stemmerLan.stem('happiness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ing'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#substring that is matching with the word has been removed here\n",
    "from nltk.stem import RegexpStemmer     \n",
    "stemmerregexp=RegexpStemmer('learn')      \n",
    "stemmerregexp.stem('learning')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'es'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmerregexp1= RegexpStemmer('match')\n",
    "stemmerregexp1.stem('matches') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'valuable'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmerregexp1= RegexpStemmer('value')     # But here it is not removing as same string is not found\n",
    "stemmerregexp1.stem('valuable') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mang'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer         #Can work in different languages\n",
    "SnowballStemmer.languages\n",
    "frenchstemmer=SnowballStemmer('french')\n",
    "frenchstemmer.stem('manges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'enjoy'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SnowballStemmer.languages\n",
    "frenchstemmer1=SnowballStemmer('english')      # Experimenting with English language\n",
    "frenchstemmer1.stem('enjoyment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porter is the least aggressive algorithm, with the description of each algorithm actually being somewhat lengthy and technical.\n",
    "\n",
    "Porter: It is the most commonly used stemmer nowadays. It is one of the few stemmers that actually have Java support and it is also the most computationally intensive of the algorithms. It is also the oldest stemming algorithm by a large margin.\n",
    "\n",
    "Snowball: This is an improvement over porter. It is slightly faster computation time than porter, with a reasonably large community around it.\n",
    "\n",
    "Lancaster: It is a very aggressive stemming algorithm. With Porter and Snowball, the stemmed representations are intuitive to a reader, not so with Lancaster, as many shorter words will become totally confusing. The fastest algorithm here, and will reduce your working set of words hugely, but if you want more distinction, not the tool you would want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am go for a walk join me\n"
     ]
    }
   ],
   "source": [
    "#stemming paragraph\n",
    "stemmer = PorterStemmer()\n",
    "example = \"i am going for a walk join me\"\n",
    "example = [stemmer.stem(token) for token in example.split(\" \")]\n",
    "print (\" \".join(example))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am quick brown fox jump ov a lazy dog\n"
     ]
    }
   ],
   "source": [
    "stemmer = LancasterStemmer()\n",
    "example = \"Am quick brown fox jumps over a lazy dog\"\n",
    "example = [stemmer.stem(token) for token in example.split(\" \")]\n",
    "print (\" \".join(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es\n"
     ]
    }
   ],
   "source": [
    "stemmerregexp1= RegexpStemmer('match')\n",
    "#stemmerregexp1.stem('matches')\n",
    "exp=\"I saw the friend whose watch matches with mine\"\n",
    "exp = stemmerregexp1.stem('matches')\n",
    "print (exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cactus\n",
      "mouse\n",
      "rock\n",
      "good\n",
      "Am\n",
      "be\n"
     ]
    }
   ],
   "source": [
    "#lemmatizer educes to actual words\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"cacti\"))   # If pos is not given it takes the default as noun\n",
    "print(lemmatizer.lemmatize(\"mice\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "\n",
    "print(lemmatizer.lemmatize(\"better\", pos = 'a')) # given the part-of-speech, better lemmatizes to good\n",
    "\n",
    "print(lemmatizer.lemmatize(\"Am\"))   \n",
    "\n",
    "print(lemmatizer.lemmatize(\"am\", pos = 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jieba\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/cb/18eeb235f833b726522d7ebed54f2278ce28ba9438e3135ab0278d9792a2/jieba-0.42.1.tar.gz (19.2MB)\n",
      "\u001b[K     |████████████████████████████████| 19.2MB 78kB/s eta 0:00:011    |██▍                             | 1.4MB 159kB/s eta 0:01:52     |████████████████▌               | 9.9MB 90kB/s eta 0:01:43     |█████████████████████▉          | 13.1MB 242kB/s eta 0:00:26     |█████████████████████████████▉  | 17.9MB 93kB/s eta 0:00:14\n",
      "\u001b[?25hBuilding wheels for collected packages: jieba\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-cp37-none-any.whl size=19314477 sha256=130881a805715e625dc70c027fa0a469d023aaa3d97c03e432dace1909567967\n",
      "  Stored in directory: /home/saurabh/.cache/pip/wheels/af/e4/8e/5fdd61a6b45032936b8f9ae2044ab33e61577950ce8e0dec29\n",
      "Successfully built jieba\n",
      "Installing collected packages: jieba\n",
      "Successfully installed jieba-0.42.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.354 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你 在 做 什么\n"
     ]
    }
   ],
   "source": [
    "#TAsk 5 Chinese Segmentation using JIEBA\n",
    "import jieba\n",
    "seg = jieba.cut(\"你在做什么\", cut_all = True)\n",
    "print(\" \".join(seg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
